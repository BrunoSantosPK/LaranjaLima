{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo: Quanto um cliente de um determinado perfil está disposto a gastar?\n",
    "\n",
    "A ideia é ter uma equação que permita alteração de parâmetros para avaliar o valor que um determinado cliente gastaria em um dado cenário (definido pelos parâmetros). Nesse sentido, nossa target é o campo \"Total Pago (R$)\".\n",
    "\n",
    "O cenário é o conjunto de parâmetros que nós previamente escolhemos para avaliar o potencial de gasto. Em outras palavras, as variáveis que utilizaremos para prever a nossa target. Dessa forma, ele será definido pelos campos:\n",
    "- Perfil do cliente\n",
    "- Categoria de item em campanha de marketing\n",
    "- Desconto total oferecido\n",
    "- Tempo desde a última compra\n",
    "- Tempo gasto para decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "BASE_PATH = \"/media/bruno/Arquivos/Desenvolvimento/LaranjaLima\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparação\n",
    "\n",
    "Como nossa target é uma variável contínua, estamos diante de um problema de regressão. Antes de acessar os métodos de treino, precisamos garantir que as variáveis categóricas sejam devidamente traduzidas em números. Técnicas de encode como OneHot, Ordinal, Target e outras são úteis nesse ponto. Um cuidado que se deve ter é com relação ao modelo que será testado. Este notebook utiliza o OneHot por dois motivos: nosso modelo principal será o linear, que lida bem com esse tipo de encoder, e queremos deixar aberta a possibilidade de avaliar o impacto no modelo de árvore.\n",
    "\n",
    "Em linhas gerais, este tratamento seria o suficiente para o processo de modelagem, uma vez que garantimos que todos os dados de entrada são numéricos. Todavia, ainda existe uma situação comum de ocorrer: campos com dados faltantes. O valor null indica que o valor não existe e pode ser gerado por alguma regra de negócio, falta de captura de informação ou um erro qualquer de processamento. De todo modo, os algoritmos de ajuste simplesmente executam cálculos, portando não vão conseguir lidar com null, afinal não é um número. Scalers são úteis nesse sentido, onde substituímos estes campos vazios por um valor padrão, que pode ser escolhido por meio de uma estratégia qualquer (número fixo, média, mediana, moda ou qualquer outro que julgar interessante). Apesar de estarmos \"inserindo\" informações que não são \"originais\" do conjunto de dados, estamos garantindo que os outros campos com números, que podem contribuir positivamente para o modelo, serão considerados no cálculo.\n",
    "\n",
    "Por fim, ainda podemos realizar um outro tipo de processamento, que é alterar a escala dos valores de entrada. Esse tipo de transformação faz com que não se gerem coeficientes (em modelos que utilizam eles) com grande variação de escala entre si. Deste modo, alinhando com técnicas de regularização, garantimos que variações na entrada não gerem uma variação descontrolada na saída, uma vez que os coeficientes tendem a serem menores e sem muita desigualdade de escala entre eles.\n",
    "\n",
    "## Processo\n",
    "\n",
    "Com os dados, tudo que precisamos fazer é invocar os métodos de ajuste de modelo e teremos um pronto para utilizar. Porém, como saber se ele é bom? Para responder essa pergunta, precisaríamos testar este modelo com dados que já conhecemos, e não foram utilizados no ajuste, para saber se o modelo é efetivo, comparando o resultado dado pelo modelo e o valor real. Assim, a estratégia é pegar a nossa base original e repartir ela em duas. Uma parte nós vamos guardar e deixar quieta, ficará \"esquecida\" enquanto trabalhamos em todo o processo de treino na parte que vamos chamar de *treino*.\n",
    "\n",
    "Em outra palavras, uma parte dos dados vamos guardar e, no momento oportuno, vamos tratar ela como se fossem dados novos que acabamos de obter. Todo o nosso processo de configuração de encode, definição de metodologia de substituição de null, padronização de valores e o ajuste do modelo serão feitos apenas com a parte de *treino*. As instâncias configuradas neste momento que serão utilizadas no fluxo de predição de novos valores, onde vamos resgatar aquela parte dos dados guardados para avaliar o quão bom o modelo foi.\n",
    "\n",
    "Nosso processo então pode ser resumido nas seguintes etapas:\n",
    "1. Carregar os dados existentes;\n",
    "2. Separar os dados em duas partes, uma para treino e outra para teste;\n",
    "3. Definir o campo que contêm a nossa target;\n",
    "4. Definir os campos das variáveis preditoras (entrada do modelo), identificando o que é categórico e o que é numérico;\n",
    "5. Ajustar as instâncias de encode, substituição e padronização utilizando apenas a base de treino;\n",
    "6. Aplicar as transformações definidas anteriormente nos campos que representam as variáveis de entrada;\n",
    "7. Instanciar o modelo que será testado, com as devidas particularidades (hyper parâmetros);\n",
    "8. Utilizar os dados transformados em (6) para ajustar o modelo instanciado em (7);\n",
    "9. Definir uma métrica de avaliação;\n",
    "10. Recuperar os dados de teste e aplicar as mesmas transformações de (6);\n",
    "11. Avaliar o resultado da métrica definida em (9) nos dados de teste e comparar com o resultado obtido no treino;\n",
    "10. Caso o modelo seja bom, utilizar em novos dados, sempre aplicando as transformações de (6) antes de utilizar o modelo.\n",
    "\n",
    "A classe criada abaixo tem como função implementar estes passos em alto nível, permitindo que o fluxo de execução se torne mais simples e o processo se torne mais fluido de executar, com as alterações mais importantes visíveis e o processo \"maçante\" encapsulado nos métodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipe:\n",
    "    def __init__(self, df: pd.DataFrame) -> None:\n",
    "        self.__base_data = df\n",
    "        self.__encoder = OneHotEncoder(sparse_output=False)\n",
    "        self.__imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "        self.__scaler = StandardScaler()\n",
    "        self.__categorical_columns: List[str] = None\n",
    "        self.__numeric_columns: List[str] = None\n",
    "        self.__target_column: str = None\n",
    "\n",
    "    def set_categorical_columns(self, columns: List[str]) -> None:\n",
    "        self.__categorical_columns = columns\n",
    "\n",
    "    def set_numeric_columns(self, columns: List[str]) -> None:\n",
    "        self.__numeric_columns = columns\n",
    "\n",
    "    def set_target_column(self, column: str) -> None:\n",
    "        self.__target_column = column\n",
    "\n",
    "    def get_categorical_columns(self) -> List[str]:\n",
    "        return [c for c in self.__categorical_columns]\n",
    "    \n",
    "    def get_numeric_columns(self) -> List[str]:\n",
    "        return [c for c in self.__numeric_columns]\n",
    "\n",
    "    def split(self, test=0.3) -> None:\n",
    "        self.train, self.test = train_test_split(self.__base_data, test_size=test, random_state=666)\n",
    "\n",
    "    def fit_preprocess(self) -> None:\n",
    "        np_cat, np_num = self.train[self.__categorical_columns].values, self.train[self.__numeric_columns].values\n",
    "        self.__encoder.fit(np_cat)\n",
    "        self.__imputer.fit(np_num)\n",
    "        self.__scaler.fit(np_num)\n",
    "\n",
    "    def transform_data(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        np_cat, np_num = df[self.__categorical_columns].values, df[self.__numeric_columns].values\n",
    "        np_cat = self.__encoder.transform(np_cat)\n",
    "        np_num = self.__imputer.transform(np_num)\n",
    "        np_num = self.__scaler.transform(np_num)\n",
    "        return np.concatenate((np_cat, np_num), axis=1)\n",
    "    \n",
    "    def train_model(self, model) -> None:\n",
    "        x_train = self.transform_data(self.train)\n",
    "        y_train = self.train[self.__target_column].values\n",
    "        model.fit(x_train, y_train)\n",
    "\n",
    "        x_test = self.transform_data(self.test)\n",
    "        y_test = self.test[self.__target_column].values\n",
    "\n",
    "        print(f\"MAE treino: {mean_absolute_error(y_train, model.predict(x_train))}\")\n",
    "        print(f\"MAE teste: {mean_absolute_error(y_test, model.predict(x_test))}\")\n",
    "\n",
    "    def predict(self, df: pd.DataFrame, model) -> np.ndarray:\n",
    "        x = self.transform_data(df)\n",
    "        return model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trade off viés e variância\n",
    "\n",
    "## Discutindo sobre os modelos testados\n",
    "\n",
    "Comentar sobre as diferenças entre os modelos, focando no custo benefício do modelo linear e da \"aproximação\" feita pelas árvores no caso de regressão.\n",
    "\n",
    "Comparar os valores da métrica no teste e no treino, falando sobre o esperado. Quando maior a diferença, maior o overfit e a variância. Comentar aqui sobre viês e variância, relacionando eles com a métrica adotada.\n",
    "\n",
    "Comentar sobre o teste de overfit utilizando um SVM polinomial, relacionando com o polinômio interpolador.\n",
    "\n",
    "Mostar o impacto do one hot no modelo de árvore, trocando o encode para target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testando modelo linear\n",
      "MAE treino: 1219.7713183373567\n",
      "MAE teste: 1260.3891670071464\n",
      "\n",
      "Testando modelo de máquinas de suporte\n",
      "MAE treino: 1246.5894557083\n",
      "MAE teste: 1388.836666042846\n",
      "\n",
      "Testando modelo de árvore de decisão\n",
      "MAE treino: 0.0\n",
      "MAE teste: 1632.0538924731184\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(f\"{BASE_PATH}/data/ml/ecommerce - python - gasto dos clientes.csv\")\n",
    "pipe = Pipe(dataset)\n",
    "pipe.split(test=0.3)\n",
    "\n",
    "pipe.set_target_column(\"Total Pago (R$)\")\n",
    "pipe.set_categorical_columns([\"Perfil do Cliente\", \"Campanha em Curso\"])\n",
    "pipe.set_numeric_columns([\n",
    "    \"Desconto Total (%)\",\n",
    "    \"Tempo desde a Última Compra (dias)\",\n",
    "    \"Tempo para Finalizar Compra (min)\"\n",
    "])\n",
    "pipe.fit_preprocess()\n",
    "\n",
    "print(\"Testando modelo linear\")\n",
    "linear_model = SGDRegressor(loss=\"squared_error\", penalty=\"l2\", alpha=0.001, eta0=0.001, learning_rate=\"invscaling\")\n",
    "pipe.train_model(linear_model)\n",
    "\n",
    "print(\"\\nTestando modelo de máquinas de suporte\")\n",
    "svm_model = SVR(kernel=\"poly\", C=1, epsilon=0.1, degree=5)\n",
    "pipe.train_model(svm_model)\n",
    "\n",
    "print(\"\\nTestando modelo de árvore de decisão\")\n",
    "dt_model = DecisionTreeRegressor()\n",
    "pipe.train_model(dt_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando o modelo\n",
    "\n",
    "Ao passarmos por todo o processo de ajuste, podemos finalmente utilizar nosso modelo para ativamente prever o comportamento de um cliente qualquer em uma situação que definirmos. Em outras palavras, podemos testar cenários diversos para um mesmo cliente. Por exemplo, podemos variar o tempo gasto para fechar a compra ou o tempo desde a última vez que ele fez alguma compra. Isso pode gerar direcionamentos para o time de marketing, que vai saber passar a ter metas numéricas para perseguir e alcançar melhores resultados.\n",
    "\n",
    "Por exemplo, tomando o nosso cliente mais \"importante\" (consome mais), será que ao reduzir o tempo de decisão na compra faz com que ele gaste mais? Sabendo que em média os consumidores deste perfil fecham suas compras com 15 minutos, conseguimos fazer eles comprarem mais se reduzirmos este tempo para 10? E se reduzirmos para 5? Assim, uma boa meta para o time de vendas poderia ser fazer com que esses clientes finalizem a compra mais rápido, esperando um aumento aproximado de R$ 100,00 por compra. Dessa forma, temos uma aplicação de ML dentro de um modelo de negócio, traçando com elas metas mais claras e objetivas. Isso é ser data driven, o que tando se fala recentemente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1777.93234979, 1816.82286749, 1855.71338519, 1229.79858893,\n",
       "       1268.68910663, 1307.57962433, 1229.79858893, 1231.75633529,\n",
       "       1233.71408166])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = pipe.get_categorical_columns()\n",
    "columns.extend(pipe.get_numeric_columns())\n",
    "cases = [\n",
    "    [\"Médio ticket e alta frequência\", \"Eletrodomésticos\", 1, 10, 15],\n",
    "    [\"Médio ticket e alta frequência\", \"Eletrodomésticos\", 1, 10, 10],\n",
    "    [\"Médio ticket e alta frequência\", \"Eletrodomésticos\", 1, 10, 5],\n",
    "\n",
    "    [\"Médio ticket e alta frequência\", \"Livros\", 1, 10, 15],\n",
    "    [\"Médio ticket e alta frequência\", \"Livros\", 1, 10, 10],\n",
    "    [\"Médio ticket e alta frequência\", \"Livros\", 1, 10, 5],\n",
    "\n",
    "    [\"Médio ticket e alta frequência\", \"Livros\", 1, 10, 15],\n",
    "    [\"Médio ticket e alta frequência\", \"Livros\", 1, 20, 15],\n",
    "    [\"Médio ticket e alta frequência\", \"Livros\", 1, 30, 15]\n",
    "]\n",
    "pipe.predict(pd.DataFrame(cases, columns=columns), linear_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
